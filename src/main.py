"""
å¯åŠ¨å…¥å£
"""
from langchain_core.runnables import Runnable
from starlette.staticfiles import StaticFiles

import logs
from agent.fay_agent import FayAgentCore
from basic import default_context
from chains import simple_chain, profile_query

_ = simple_chain, profile_query


def history_chat_build(history):
    # å†å²å¯¹è¯æ€»æ˜¯ä»ç”¨æˆ·å¼€å§‹ï¼Œç„¶åæœºå™¨äºº
    history_msg = []
    history = history[-default_context['remember_history']:]
    for i, (user_message, bot_message) in enumerate(history):
        if isinstance(user_message, list):
            user_message = "".join(user_message)
        history_msg.append(('user', user_message))
        if isinstance(bot_message, list):
            bot_message = "".join(bot_message)
        history_msg.append(('ai', bot_message))
    history_msg = str(history_msg).replace(r"\n", "\n")
    return history_msg


def profile_rag_msg(message, history):
    history_msg = history_chat_build(history)
    pro_chain: Runnable = default_context['profile_query']
    res = pro_chain.invoke({'chat_history': history_msg, 'question': message})
    if res['stop']:
        yield res['profile'] + "ğŸ¥°"
    else:
        partial_message = ""
        if res['keywords']:
            tips = f"> ğŸ¤” å…³é”®è¯ : **{res['keywords']}**\n\n"
            partial_message += tips
            yield partial_message
        retriever_chain: Runnable = default_context['retriever_chain']

        for chunk in retriever_chain.stream(res):
            partial_message += chunk.content
            yield partial_message
        yield partial_message + "ğŸ¥°"


def profile_rag(message, history):
    chain: Runnable = default_context['profile_query_rag']
    history_msg = history_chat_build(history)
    partial_message = ""
    logs.get_logger('chat').debug(history_msg)
    for chunk in chain.stream({"question": message, "chat_history": history_msg}):
        partial_message = partial_message + chunk.content
        yield partial_message
    yield partial_message + "ğŸ¥°"


def simple_rag(message, history):
    chain: Runnable = default_context['simple_chain']
    # å†å²å¯¹è¯æ€»æ˜¯ä»ç”¨æˆ·å¼€å§‹ï¼Œç„¶åæœºå™¨äºº
    history_msg = history_chat_build(history)
    partial_message = ""
    logs.get_logger('chat').debug(history_msg)
    for chunk in chain.stream([message, history_msg]):
        partial_message = partial_message + chunk.content
        yield partial_message
    yield partial_message + "ğŸ¥°"


def simple_chat(message, history):
    chain: Runnable = default_context['ServeChatModel']
    # å†å²å¯¹è¯æ€»æ˜¯ä»ç”¨æˆ·å¼€å§‹ï¼Œç„¶åæœºå™¨äºº
    history_msg = history_chat_build(history)
    partial_message = ""
    logs.get_logger('chat').debug(history_msg)
    for chunk in chain.stream([history_msg, message]):
        partial_message = partial_message + chunk.content
        yield partial_message


agent = FayAgentCore()


def simple_agent(message, history):
    user_input = message
    return agent.run(user_input, agent.qdrant_retriever)[1]


def retriever_test(message, history):
    retriever = default_context['DocRetriever']
    re = retriever.invoke(message)
    if len(re) > 0:
        res = '\n\n---\n\n## '.join([doc.page_content for doc in re])
    else:
        return "# ğŸ¥¹ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
    msg = f'# ğŸ¥°ã€{message}ã€‘çš„æ£€ç´¢ç»“æœ\n\n##' + res
    return msg


# å…¬å¸åç§°
company_name = "WeYon"
con_limit = 20

import gradio as gr

config = {
    "theme": "soft",
    "submit_btn": "å‘é€",
    "retry_btn": "é‡è¯•",
    "undo_btn": "æ’¤å›å¹¶é‡æ–°ç¼–è¾‘",
    "clear_btn": "æ–°å»ºå¯¹è¯",
    "stop_btn": "åœæ­¢",
    "chatbot": gr.Chatbot(placeholder="Powered By WeYon AI Department", likeable=True, label="Chatbot", scale=1,
                          height=200),
    "textbox": gr.Textbox(placeholder="ä¸WeYon AIå¯¹è¯", scale=8),
}

if __name__ == "__main__":
    rag_interface = gr.ChatInterface(simple_rag, title=f"{company_name} Question Rag", concurrency_limit=con_limit,
                                     description=f"{company_name} åŸºäºé—®é¢˜æ£€ç´¢å¯¹è¯",
                                     **config)
    profile_interface = gr.ChatInterface(profile_rag_msg, title=f"{company_name} Keywords Rag",
                                         concurrency_limit=con_limit,
                                         description=f"{company_name} åŸºäºå…³é”®è¯æ£€ç´¢",
                                         **config
                                         )
    chat_interface = gr.ChatInterface(simple_chat, title=f"{company_name} Chat", concurrency_limit=con_limit,
                                      description=f"{company_name} ç›´æ¥ä¸æ¨¡å‹å¯¹è¯",
                                      **config)
    agent_interface = gr.ChatInterface(simple_agent, title=f"{company_name} Agent", concurrency_limit=con_limit,
                                       description=f"{company_name} æŸ¥è¯¢æ•°æ®åº“çš„æ™ºèƒ½ä½“",
                                       **config)
    retriever_test_interface = gr.ChatInterface(fn=retriever_test,
                                                title=f"{company_name} Retrieverå‘½ä¸­ç‡æµ‹è¯•",
                                                description="ç”¨æ¥æµ‹è¯•Retrieverå‘½ä¸­ç‡ï¼Œç”Ÿäº§ç¯å¢ƒè®°å¾—å…³é—­ï¼ï¼å½“å‰æµ‹è¯•å¯¹è±¡ä¸ºDocRetriever",
                                                **config)

    from fastapi import FastAPI

    app = FastAPI()
    gr.mount_gradio_app(app, rag_interface, path="/rag")
    gr.mount_gradio_app(app, profile_interface, path="/profile/")
    gr.mount_gradio_app(app, chat_interface, path="/chat")
    gr.mount_gradio_app(app, agent_interface, path="/agent")
    gr.mount_gradio_app(app, retriever_test_interface, path="/retriever")

    app.mount("/", StaticFiles(directory="./pages", html=True), name="pages")

    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=7860)
